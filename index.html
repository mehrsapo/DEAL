<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <title>DEALing with Image Reconstruction</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="style.css">
</head>
<body>

  <header>
    <h1>DEALing with Image Reconstruction: Deep Attentive Least Squares</h1>
    <h2>ICML 2025</h2>
    <p><strong>Mehrsa Pourya</strong>, Erich Kobler, Michael Unser, Sebastian Neumayer</p>
    <p>
      <a href="https://arxiv.org/abs/2502.04079" target="_blank">[Paper]</a>
      <a href="https://github.com/mehrsapo/DEAL" target="_blank">[Code]</a>
    </p>
  </header>

  <main>
    <section>
      <h3>Abstract</h3>
      <p>
        DEAL is a novel and universal image reconstruction framework that combines the interpretability of classical methods with the power of deep learning.
        What I find most exciting is that the mapping from measurements to the reconstructed image remains highly interpretable:
        each output pixel is computed as an attentive average of the (adjoint of) the measurements, with weights that adapt to the spatial structure of the image.
      </p>
    </section>

    <section>
      <h3>Figure</h3>
      <img src="static/images/figure8.png" alt="Figure 8" style="max-width:90%;margin-top:1em;border-radius:8px;">
      <p><em>Figure 8: Visualizing how each output pixel is a spatially adaptive weighted average of the input measurements.</em></p>
    </section>

    <section>
      <h3>BibTeX</h3>
      <pre><code>@article{pourya2025dealing,
  title={DEALing with Image Reconstruction: Deep Attentive Least Squares},
  author={Pourya, Mehrsa and Kobler, Erich and Unser, Michael and Neumayer, Sebastian},
  journal={arXiv preprint arXiv:2502.04079},
  year={2025}
}</code></pre>
    </section>
  </main>

</body>
</html>
